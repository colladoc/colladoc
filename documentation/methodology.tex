\section{Methodology}\label{sec:methodology}
% section outcomes

This section describes the methodologies and techniques we have used to complete the various tasks and challenges we encountered throughout the project.

{Planning}

Strategically planning the division of the overall task into manageable sub-problems was essential to the project’s success. Our overall strategy for the project was to achieve a working product with the core functionality required as early as possible and then to focus on implementing features that enhance this core functionality.

One technique that we found very useful was brainstorming - it helped to bring out all ideas that we individually had about the project and to structure and reconcile them. We did brainstorming about the project scope and specification; its use cases and the contents of the three projects reports.

We decided to divide the tasks based on the architecture layer that they belonged to. We set one task per day for each team member, the task was small enough to be completed in that time. We balanced the project tasks with other assignments that each team member had. We initially focused on the Model Layer tasks, extending to Query and UI Layers later. After the first few weeks we worked on all layers in parallel. 

\emph{\color{red}For more details about the project plan} refer to the Logbook in the Appendix.

\subsection{Development Approach}

Throughout the course of the project we used an iterative development approach. More specifically, we incorporated key practices from agile development, such as iterations, scrum meetings and pair programming and tailored them to suit our needs.

Our work was done in a series of iterations (two iterations per week). At the start of every iteration, we had a retrospective discussion on each of the work items from the previous iteration and then planned the work that each team member would do during the next iteration. We had a detailed project schedule which included the daily tasks and progress of each team member. The Logbook in the Appendix is based on it. 

The team met at least once during each iteration to work together on the tasks that were assigned. In order to improve the development process further, we set automatic mail notification with each code check-in.

At the end of every two iterations, we gave a product demonstration to our supervisors to show the progress that had been made during the week. The team found this approach to be very efficient since it structured our schedule in a way that ensured regular team meetings and continuous communication. Getting feedback early on prevented us from wasting time implementing inessential requirements and allowed us to extend the product with new useful functionalities. Keeping minutes for each of our meetings helped us to log and act on all feedback. Scheduling regular demonstrations also helped the team to focus on having an iteratively fully working product throughout the development. Additionally, we have found that this iterative style of development helped us to split our overall project goal into a series of manageable sub-tasks and sub-goals.

In addition to iterations, the team also defined a set of three product milestones, which are specific dates where we expected the project to have a certain set of features. These higher-level divisions in the project schedule were very helpful in setting a specific context for the planning of iterations. 

\subsection{Quality Assurance}

Setting up the testing infrastructure was one of our first tasks. We have different types of tests for our product:
\begin{itemize}
\item Query Parser Unit Tests – The tests are written in Scala using JUnit.
\item Model Unit Tests – We used Scala Specs, JUnit and JMock. Specs is a library that allows expressing unit tests more naturally. We used JMock to isolate our tests from the real model objects generated by the Scala compiler. The tests check that the given model (package, class, member, etc) is correctly stored in the Lucene index.
\item Integration UI Tests. One of our first tasks was to create a proof-of-concept UI test using the Selenium IDE and integrate the test into our codebase. The UI is the only part of the system that has no individual unit tests but we believe that integration tests combined with unit tests for the Model and Query gave us confidence in our work. The Integration tests can be run from the project build tool.
\end{itemize}

A demo / test project was created that serves as test data for the Integration tests. Its contents are meant to include a variety of cases – deep class hierarchies, methods with similar signatures, similarly named types, different types of comments (class, member, parameter, return type, code example comments). Both positive and negative tests can be created without searching for a particular structure or arrangement in a real project. The demo project is updated when new test cases are identified.

There were no tests for the original Colladoc, but we decided not to test the original individually because our Integration tests cover the interaction points between our extension and the original. Furthermore, we only made minimal changes to the original Colladoc code.

We used code coverage metrics, generated by the Scala Code Coverage Tool (SCCT), to evaluate the depth of coverage that our tests provided. This helped us to identify where additional tests were needed.

We ran the Colladoc Smart Search on the largest Scala codebases we could find (Lift Web Framework and the Scala Library) during the second milestone and this helped ensure that our system scaled well to larger codebases.
\subsection{Overcoming Challenges}

We were able to overcome the intellectual and technical problems we faced during the project. Although the learning curve for the project language - Scala and the project related tools was quite steep, we organized regular presentations on what we have learned so far. This involved all team members and sped up the learning process. 
We faced several challenges during the project (initial project configuration, running the project on large code base), by persevering and working together.

We were also able to identify and mitigate high-risk project requirements and tasks so that the schedule was not jeopardised. In particular, some of the more complex query syntax ([q15]) that was specified was clearly high-risk due to the amount of work required in both the model layer and the query layer to support it. The team mitigated the risk posed by high-risk features by prioritizing the features we would implement first. More specifically, we implemented complex features and query syntax that were essential requirements (q[1] - q[5])  early on in development so that we would have a more accurate estimate of the amount of work required and could plan accordingly. In contrast, complex features and query syntax that were non-essential requirements were scheduled later in development so that they did not delay work on any essential requirements.

Another risk was how well the indexing and search would scale to large codebases. To mitigate them we ran performance investigations as early as possible.

The design decision to take a dependency on Lucene 4.0 instead of Lucene 3.0 was another high-risk task that we managed by scheduling work that we could fall-back on in case we found that Lucene 4.0 integration was not feasible. (For more details, refer to Implementation - Lucene 3.0 or 4.0)
